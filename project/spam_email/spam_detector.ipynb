{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef1a5572",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.optim import AdamW\n",
    "from transformers import BertTokenizer, BertForSequenceClassification, get_linear_schedule_with_warmup\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, confusion_matrix, classification_report, roc_curve, precision_recall_curve\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from tqdm import tqdm\n",
    "import re\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "\n",
    "try:\n",
    "    import google.colab\n",
    "    IN_COLAB = True\n",
    "    import subprocess\n",
    "    import sys\n",
    "    for pkg in [\"transformers==4.35.0\", \"datasets==2.14.0\", \"accelerate==0.24.0\"]:\n",
    "        subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"-q\", pkg])\n",
    "    \n",
    "    # Search local paths first\n",
    "    dataset_paths = ['dataset/', '../dataset/', 'project/spam_email/dataset/', '/content/project/spam_email/dataset/']\n",
    "    dataset_dir = None\n",
    "    for path in dataset_paths:\n",
    "        if os.path.exists(path) and len([f for f in os.listdir(path) if f.endswith('.csv')]) >= 3:  # Need 3 CSV files\n",
    "            dataset_dir = path\n",
    "            break\n",
    "    \n",
    "    if not dataset_dir:\n",
    "        from google.colab import drive\n",
    "        drive.mount('/content/drive')\n",
    "        GOOGLE_DRIVE_DATASET_PATH = '/content/drive/MyDrive/ml_dataset/'\n",
    "        if os.path.exists(GOOGLE_DRIVE_DATASET_PATH) and len([f for f in os.listdir(GOOGLE_DRIVE_DATASET_PATH) if f.endswith('.csv')]) >= 3:\n",
    "            dataset_dir = GOOGLE_DRIVE_DATASET_PATH\n",
    "        if not dataset_dir:\n",
    "            raise FileNotFoundError(f\"Dataset not found at {GOOGLE_DRIVE_DATASET_PATH}\")\n",
    "    \n",
    "    globals()['DATASET_DIR'] = dataset_dir\n",
    "    from torch.cuda.amp import autocast, GradScaler  # Mixed precision training\n",
    "    from tqdm.notebook import tqdm as notebook_tqdm\n",
    "    tqdm = notebook_tqdm\n",
    "    import gc\n",
    "    def clear_memory():  # GPU memory cleanup\n",
    "        if torch.cuda.is_available():\n",
    "            torch.cuda.empty_cache()\n",
    "        gc.collect()\n",
    "except ImportError:\n",
    "    IN_COLAB = False\n",
    "    globals()['DATASET_DIR'] = 'dataset/'\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "torch.manual_seed(42)  # Reproducibility\n",
    "np.random.seed(42)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(42)\n",
    "\n",
    "print(f\"Device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "246f9f00",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_dir = globals().get('DATASET_DIR', 'dataset/')\n",
    "\n",
    "train1 = pd.read_csv(os.path.join(dataset_dir, 'spam_train1.csv'))\n",
    "train1_clean = train1[['v1', 'v2']].copy()  # v1=label, v2=text\n",
    "train1_clean.columns = ['label', 'text']\n",
    "train1_clean = train1_clean.dropna()\n",
    "\n",
    "train2 = pd.read_csv(os.path.join(dataset_dir, 'spam_train2.csv'))\n",
    "train2_clean = train2[['label', 'text']].copy()\n",
    "train2_clean = train2_clean.dropna()\n",
    "\n",
    "train1_counts = train1_clean['label'].value_counts()\n",
    "train2_counts = train2_clean['label'].value_counts()\n",
    "print(\"TrainData1 counts ->\", train1_counts.to_dict())\n",
    "print(\"TrainData2 counts ->\", train2_counts.to_dict())\n",
    "\n",
    "combined_raw = pd.concat([train1_clean, train2_clean], ignore_index=True)\n",
    "print(f\"Combined samples: {len(combined_raw)}\")\n",
    "print(\"Combined counts ->\", combined_raw['label'].value_counts().to_dict())\n",
    "\n",
    "combined_stats = combined_raw['label'].value_counts().rename(index={'ham': 'Ham', 'spam': 'Spam'})\n",
    "print(\"Combined dataset counts ->\", combined_stats.to_dict())\n",
    "\n",
    "data = combined_raw.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "015282f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "STOPWORDS = {'the', 'a', 'an', 'and', 'or', 'but', 'in', 'on', 'at', 'to', 'for', 'of', 'with', 'by', 'from', 'is', 'was', 'are', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'do', 'does', 'did', 'will', 'would', 'could', 'should', 'may', 'might', 'must', 'can', 'this', 'that', 'these', 'those', 'i', 'you', 'he', 'she', 'it', 'we', 'they', 'them', 'their', 'what', 'which', 'who', 'when', 'where', 'why', 'how'}\n",
    "\n",
    "def clean_text(text):\n",
    "    if pd.isna(text):\n",
    "        return \"\"\n",
    "    text = str(text).lower()  # Normalize to lowercase\n",
    "    text = re.sub(r'<[^>]+>', '', text)  # Remove HTML tags\n",
    "    text = re.sub(r'[^a-zA-Z0-9\\\\s]', ' ', text)  # Keep only alphanumeric and spaces\n",
    "    text = re.sub(r'\\\\s+', ' ', text).strip()  # Normalize whitespace\n",
    "    words = [w for w in text.split() if w not in STOPWORDS]  # Remove stopwords\n",
    "    return ' '.join(words)\n",
    "\n",
    "data['text_clean'] = data['text'].apply(clean_text)\n",
    "data['label_num'] = data['label'].map({'ham': 0, 'spam': 1})\n",
    "data = data.dropna(subset=['label_num'])\n",
    "data = data[data['text_clean'].str.len() > 0]  # Remove empty texts\n",
    "\n",
    "print(f\"After cleaning: {len(data)} samples\")\n",
    "print(f\"Ham: {sum(data['label_num']==0)}, Spam: {sum(data['label_num']==1)}\")\n",
    "print(f\"Removed during cleaning: {len(combined_raw) - len(data)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7210c95",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = data['text_clean'].values\n",
    "y = data['label_num'].values\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)  # 80/20 split, maintain class balance\n",
    "\n",
    "class_weights = compute_class_weight('balanced', classes=np.unique(y_train), y=y_train)  # Handle class imbalance\n",
    "class_weights = torch.FloatTensor(class_weights).to(device)\n",
    "\n",
    "print(f\"Train: {len(X_train)}, Val: {len(X_val)}\")\n",
    "print(f\"Class weights: {class_weights.cpu().numpy()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a77023e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "def tokenize(texts, tokenizer, max_len=128):\n",
    "    input_ids = []\n",
    "    attention_masks = []\n",
    "    for text in tqdm(texts, desc=\"Tokenizing\"):\n",
    "        encoded = tokenizer.encode_plus(text, add_special_tokens=True, max_length=max_len,\n",
    "                                       padding='max_length', truncation=True, return_attention_mask=True, return_tensors='pt')\n",
    "        input_ids.append(encoded['input_ids'].flatten())\n",
    "        attention_masks.append(encoded['attention_mask'].flatten())  # Mask padding tokens\n",
    "    return torch.stack(input_ids), torch.stack(attention_masks)\n",
    "\n",
    "train_ids, train_masks = tokenize(X_train, tokenizer)\n",
    "val_ids, val_masks = tokenize(X_val, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "195f237e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SpamDataset(Dataset):\n",
    "    \"\"\"PyTorch dataset for BERT input format\"\"\"\n",
    "    def __init__(self, ids, masks, labels):\n",
    "        self.ids = ids\n",
    "        self.masks = masks\n",
    "        self.labels = labels\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "    def __getitem__(self, idx):\n",
    "        return {'input_ids': self.ids[idx], 'attention_mask': self.masks[idx],\n",
    "                'labels': torch.tensor(self.labels[idx], dtype=torch.long)}\n",
    "\n",
    "#create datasets and loaders \n",
    "batch_size = 8\n",
    "train_dataset = SpamDataset(train_ids, train_masks, torch.tensor(y_train, dtype=torch.long))\n",
    "val_dataset = SpamDataset(val_ids, val_masks, torch.tensor(y_val, dtype=torch.long))\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7126fdcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = BertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=2).to(device)\n",
    "lr = 2e-5\n",
    "epochs = 5\n",
    "optimizer = AdamW(model.parameters(), lr=lr, eps=1e-8)\n",
    "scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=0, num_training_steps=len(train_loader) * epochs)  # Linear LR decay\n",
    "\n",
    "use_amp = IN_COLAB and torch.cuda.is_available()  # Enable mixed precision on Colab GPU\n",
    "if use_amp:\n",
    "    scaler = GradScaler()\n",
    "\n",
    "print(f\"Epochs: {epochs}, LR: {lr}, Batch: {batch_size}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86f2e1c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(model, loader, optimizer, scheduler, device, class_weights):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for batch in tqdm(loader, desc=\"Training\"):\n",
    "        ids = batch['input_ids'].to(device)\n",
    "        mask = batch['attention_mask'].to(device)\n",
    "        labels = batch['labels'].to(device)\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        if use_amp:  # Mixed precision for faster training \n",
    "            with autocast():\n",
    "                outputs = model(input_ids=ids, attention_mask=mask, labels=labels)\n",
    "                loss = outputs.loss\n",
    "            total_loss += loss.item()\n",
    "            scaler.scale(loss).backward()\n",
    "            scaler.unscale_(optimizer)\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)  # Gradient clipping\n",
    "            scaler.step(optimizer)\n",
    "            scaler.update()\n",
    "        else:\n",
    "            outputs = model(input_ids=ids, attention_mask=mask, labels=labels)\n",
    "            loss = outputs.loss\n",
    "            total_loss += loss.item()\n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "            optimizer.step()\n",
    "        if scheduler:\n",
    "            scheduler.step()\n",
    "    return total_loss / len(loader)\n",
    "\n",
    "def evaluate(model, loader, device):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    preds = []\n",
    "    labels = []\n",
    "    probs = []\n",
    "    with torch.no_grad():  # Disable gradient computation\n",
    "        for batch in tqdm(loader, desc=\"Evaluating\"):\n",
    "            ids = batch['input_ids'].to(device)\n",
    "            mask = batch['attention_mask'].to(device)\n",
    "            label = batch['labels'].to(device)\n",
    "            outputs = model(input_ids=ids, attention_mask=mask, labels=label)\n",
    "            total_loss += outputs.loss.item()\n",
    "            logits = outputs.logits\n",
    "            prob = torch.softmax(logits, dim=1)\n",
    "            preds.extend(torch.argmax(logits, dim=1).cpu().numpy())\n",
    "            labels.extend(label.cpu().numpy())\n",
    "            probs.extend(prob[:, 1].cpu().numpy())  # Spam class probability\n",
    "    \n",
    "    loss = total_loss / len(loader)\n",
    "    acc = accuracy_score(labels, preds)\n",
    "    prec = precision_score(labels, preds)\n",
    "    rec = recall_score(labels, preds)\n",
    "    f1 = f1_score(labels, preds)\n",
    "    auc = roc_auc_score(labels, probs)\n",
    "    return loss, acc, prec, rec, f1, auc, preds, labels, probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bc26461",
   "metadata": {},
   "outputs": [],
   "source": [
    "stats = []\n",
    "best_f1 = 0\n",
    "best_state = None\n",
    "\n",
    "print(\"Training\")\n",
    "print(\"---\")\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    if IN_COLAB:\n",
    "        clear_memory()  # Free GPU memory between epochs\n",
    "    \n",
    "    train_loss = train_epoch(model, train_loader, optimizer, scheduler, device, class_weights)\n",
    "    val_loss, acc, prec, rec, f1, auc, preds, labels, val_probs = evaluate(model, val_loader, device)\n",
    "    \n",
    "    if f1 > best_f1:  # Save best model based on F1 score\n",
    "        best_f1 = f1\n",
    "        best_state = model.state_dict().copy()\n",
    "    \n",
    "    stats.append({'epoch': epoch + 1, 'train_loss': train_loss, 'val_loss': val_loss,\n",
    "                  'acc': acc, 'prec': prec, 'rec': rec, 'f1': f1, 'auc': auc})\n",
    "    \n",
    "    print(f\"Epoch {epoch+1}/{epochs} | Loss: {train_loss:.4f}/{val_loss:.4f} | Acc: {acc:.4f} | Prec: {prec:.4f} | Rec: {rec:.4f} | F1: {f1:.4f} | AUC: {auc:.4f}\")\n",
    "\n",
    "model.load_state_dict(best_state)  # Load best model weights\n",
    "print(\"\\\\n---\")\n",
    "print(f\"Best F1: {best_f1:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e2d4653",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tune_hyperparams():\n",
    "    param_grid = {\n",
    "        'lr': [1e-5, 2e-5, 3e-5],\n",
    "        'epochs': [2, 3, 5],\n",
    "        'batch_size': [8, 16],\n",
    "        'max_len': [128, 256]\n",
    "    }\n",
    "    \n",
    "    best_score = 0\n",
    "    best_params = None\n",
    "    results = []\n",
    "    \n",
    "    print(\"Hyperparameter Search\")\n",
    "    print(\"---\")\n",
    "    \n",
    "    # Use subset for faster search\n",
    "    sample_size = min(1000, len(X_train))\n",
    "    X_sample = X_train[:sample_size]\n",
    "    y_sample = y_train[:sample_size]\n",
    "    \n",
    "    for lr in param_grid['lr']:\n",
    "        for ep in param_grid['epochs']:\n",
    "            for bs in param_grid['batch_size']:\n",
    "                for ml in param_grid['max_len']:\n",
    "                    print(f\"Testing: lr={lr}, epochs={ep}, batch={bs}, max_len={ml}\")\n",
    "                    \n",
    "                    sample_ids, sample_masks = tokenize(X_sample, tokenizer, ml)\n",
    "                    sample_dataset = SpamDataset(sample_ids, sample_masks, torch.tensor(y_sample))\n",
    "                    sample_loader = DataLoader(sample_dataset, batch_size=bs, shuffle=True)\n",
    "                    \n",
    "                    test_model = BertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=2).to(device)\n",
    "                    test_opt = AdamW(test_model.parameters(), lr=lr, eps=1e-8)\n",
    "                    \n",
    "                    for _ in range(ep):\n",
    "                        train_epoch(test_model, sample_loader, test_opt, None, device, class_weights)\n",
    "                    \n",
    "                    _, _, _, _, f1_score_val, _, _, _, _ = evaluate(test_model, val_loader, device)\n",
    "                    \n",
    "                    results.append({'lr': lr, 'epochs': ep, 'batch': bs, 'max_len': ml, 'f1': f1_score_val})\n",
    "                    \n",
    "                    if f1_score_val > best_score:\n",
    "                        best_score = f1_score_val\n",
    "                        best_params = {'lr': lr, 'epochs': ep, 'batch': bs, 'max_len': ml}\n",
    "                    \n",
    "                    print(f\"F1: {f1_score_val:.4f}\")\n",
    "                    print()\n",
    "    \n",
    "    print(\"---\")\n",
    "    print(f\"Best params: {best_params}\")\n",
    "    print(f\"Best F1: {best_score:.4f}\")\n",
    "    return best_params, results\n",
    "\n",
    "# Comment out to disable hyperparameter search\n",
    "best_params, hp_results = tune_hyperparams()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67943b8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_loss, acc, prec, rec, f1, auc, preds, labels, val_probs = evaluate(model, val_loader, device)\n",
    "\n",
    "print(\"Final Metrics\")\n",
    "print(\"---\")\n",
    "print(f\"Accuracy: {acc:.4f}\")\n",
    "print(f\"Precision: {prec:.4f}\")\n",
    "print(f\"Recall: {rec:.4f}\")\n",
    "print(f\"F1: {f1:.4f}\")\n",
    "print(f\"ROC-AUC: {auc:.4f}\")\n",
    "print()\n",
    "print(classification_report(labels, preds, target_names=['Ham', 'Spam']))\n",
    "\n",
    "val_total = len(labels)\n",
    "val_ham = int((np.array(labels) == 0).sum())\n",
    "val_spam = val_total - val_ham\n",
    "print(f\"Validation totals -> Total: {val_total}, Ham: {val_ham}, Spam: {val_spam}\")\n",
    "\n",
    "cm = confusion_matrix(labels, preds)\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=['Ham', 'Spam'], yticklabels=['Ham', 'Spam'])\n",
    "plt.title('Confusion Matrix')\n",
    "plt.ylabel('True')\n",
    "plt.xlabel('Predicted')\n",
    "plt.tight_layout()\n",
    "plt.savefig('confusion_matrix.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "# Training history plots\n",
    "if stats:\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "    epochs_range = [stat['epoch'] for stat in stats]\n",
    "    \n",
    "    axes[0, 0].plot(epochs_range, [stat['train_loss'] for stat in stats], 'b-', label='Training Loss')\n",
    "    axes[0, 0].plot(epochs_range, [stat['val_loss'] for stat in stats], 'r-', label='Validation Loss')\n",
    "    axes[0, 0].set_title('Training and Validation Loss')\n",
    "    axes[0, 0].set_xlabel('Epoch')\n",
    "    axes[0, 0].set_ylabel('Loss')\n",
    "    axes[0, 0].legend()\n",
    "    \n",
    "    axes[0, 1].plot(epochs_range, [stat['acc'] for stat in stats], 'g-', label='Accuracy')\n",
    "    axes[0, 1].set_title('Validation Accuracy')\n",
    "    axes[0, 1].set_xlabel('Epoch')\n",
    "    axes[0, 1].set_ylabel('Accuracy')\n",
    "    axes[0, 1].legend()\n",
    "    \n",
    "    axes[1, 0].plot(epochs_range, [stat['f1'] for stat in stats], 'm-', label='F1 Score')\n",
    "    axes[1, 0].set_title('Validation F1 Score')\n",
    "    axes[1, 0].set_xlabel('Epoch')\n",
    "    axes[1, 0].set_ylabel('F1 Score')\n",
    "    axes[1, 0].legend()\n",
    "    \n",
    "    axes[1, 1].plot(epochs_range, [stat['auc'] for stat in stats], 'c-', label='ROC-AUC')\n",
    "    axes[1, 1].set_title('Validation ROC-AUC')\n",
    "    axes[1, 1].set_xlabel('Epoch')\n",
    "    axes[1, 1].set_ylabel('ROC-AUC')\n",
    "    axes[1, 1].legend()\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig('training_history.png', dpi=300, bbox_inches='tight')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bdbfe2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Additional validation visualizations\n",
    "val_counts = pd.Series(labels).value_counts().rename({0: 'Ham', 1: 'Spam'})\n",
    "pred_counts = pd.Series(preds).value_counts().rename({0: 'Ham', 1: 'Spam'})\n",
    "\n",
    "print(\"Validation label distribution ->\", val_counts.to_dict())\n",
    "print(\"Validation prediction distribution ->\", pred_counts.to_dict())\n",
    "\n",
    "fpr, tpr, _ = roc_curve(labels, val_probs)\n",
    "prec_curve, rec_curve, _ = precision_recall_curve(labels, val_probs)\n",
    "\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
    "axes[0].bar(val_counts.index, val_counts.values, color=['#4c72b0', '#dd8452'])\n",
    "axes[0].set_title('Validation Labels')\n",
    "axes[0].set_ylabel('Count')\n",
    "\n",
    "axes[1].plot(fpr, tpr, color='#4c72b0')\n",
    "axes[1].plot([0, 1], [0, 1], '--', color='gray')\n",
    "axes[1].set_title('ROC Curve')\n",
    "axes[1].set_xlabel('False Positive Rate')\n",
    "axes[1].set_ylabel('True Positive Rate')\n",
    "axes[1].set_xlim([0, 1])\n",
    "axes[1].set_ylim([0, 1])\n",
    "\n",
    "axes[2].plot(rec_curve, prec_curve, color='#dd8452')\n",
    "axes[2].set_title('Precision-Recall Curve')\n",
    "axes[2].set_xlabel('Recall')\n",
    "axes[2].set_ylabel('Precision')\n",
    "axes[2].set_xlim([0, 1])\n",
    "axes[2].set_ylim([0, 1])\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('validation_curves.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "# Probability distribution by true label\n",
    "prob_df = pd.DataFrame({'prob': val_probs, 'label': labels})\n",
    "prob_df['label_name'] = prob_df['label'].map({0: 'Ham', 1: 'Spam'})\n",
    "plt.figure(figsize=(8, 5))\n",
    "sns.kdeplot(data=prob_df, x='prob', hue='label_name', fill=True, common_norm=False, palette=['#4c72b0', '#dd8452'], alpha=0.4)\n",
    "plt.title('Spam Probability Distribution by True Label')\n",
    "plt.xlabel('Spam probability')\n",
    "plt.ylabel('Density')\n",
    "plt.savefig('validation_probability_density.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8907f0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test set visualizations and summary\n",
    "print(\"Test summary\")\n",
    "print(\"---\")\n",
    "test_counts = results['prediction_label'].value_counts().reindex(['ham', 'spam'], fill_value=0)\n",
    "print(test_counts.rename(index={'ham': 'Ham', 'spam': 'Spam'}).to_dict())\n",
    "\n",
    "plt.figure(figsize=(6, 4))\n",
    "sns.barplot(x=test_counts.index.map({'ham': 'Ham', 'spam': 'Spam'}), y=test_counts.values, palette=['#4c72b0', '#dd8452'])\n",
    "plt.title('Test Predictions')\n",
    "plt.ylabel('Count')\n",
    "plt.tight_layout()\n",
    "plt.savefig('test_prediction_counts.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "# Compare validation vs test distributions\n",
    "val_counts = pd.Series(labels).value_counts().rename({0: 'Ham', 1: 'Spam'})\n",
    "test_counts_named = test_counts.rename(index={'ham': 'Ham', 'spam': 'Spam'})\n",
    "compare_df = pd.DataFrame({'Validation': val_counts, 'Test': test_counts_named}).fillna(0)\n",
    "compare_df.plot(kind='bar', figsize=(6, 4), color=['#55a868', '#c44e52'])\n",
    "plt.title('Validation vs Test Label Counts')\n",
    "plt.ylabel('Count')\n",
    "plt.tight_layout()\n",
    "plt.savefig('validation_vs_test_counts.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "# Probability distribution for test predictions\n",
    "test_prob_df = pd.DataFrame({'prob': results['spam_probability'], 'label': results['prediction_label'].map({'ham': 0, 'spam': 1})})\n",
    "test_prob_df['label_name'] = test_prob_df['label'].map({0: 'Ham', 1: 'Spam'})\n",
    "plt.figure(figsize=(8, 5))\n",
    "sns.kdeplot(data=test_prob_df, x='prob', hue='label_name', fill=True, common_norm=False, palette=['#4c72b0', '#dd8452'], alpha=0.4)\n",
    "plt.title('Test Spam Probability Distribution by Predicted Label')\n",
    "plt.xlabel('Spam probability')\n",
    "plt.ylabel('Density')\n",
    "plt.savefig('test_probability_density.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "# Display a few sample predictions\n",
    "pd.options.display.max_colwidth = 120\n",
    "display(results[['text', 'prediction_label', 'spam_probability', 'confidence']].head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd1fa81a",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_file = os.path.join(dataset_dir, 'spam_test.csv')\n",
    "test_data = pd.read_csv(test_file, header=None, names=['text'])\n",
    "test_texts = test_data['text'].apply(clean_text).values\n",
    "test_ids, test_masks = tokenize(test_texts, tokenizer)\n",
    "test_dataset = SpamDataset(test_ids, test_masks, torch.zeros(len(test_texts)))  # Dummy labels for dataset\n",
    "test_loader = DataLoader(test_dataset, batch_size=16, shuffle=False)\n",
    "\n",
    "model.eval()\n",
    "predictions = []\n",
    "probabilities = []\n",
    "with torch.no_grad():\n",
    "    for batch in tqdm(test_loader, desc=\"Predicting\"):\n",
    "        outputs = model(input_ids=batch['input_ids'].to(device), attention_mask=batch['attention_mask'].to(device))\n",
    "        probs = torch.softmax(outputs.logits, dim=1)\n",
    "        predictions.extend(torch.argmax(outputs.logits, dim=1).cpu().numpy())\n",
    "        probabilities.extend(probs[:, 1].cpu().numpy())  # Spam class probability\n",
    "\n",
    "results = pd.DataFrame({\n",
    "    'text': test_data['text'].values,\n",
    "    'text_clean': test_texts,\n",
    "    'prediction': predictions,\n",
    "    'prediction_label': ['spam' if p == 1 else 'ham' for p in predictions],\n",
    "    'spam_probability': probabilities,\n",
    "    'confidence': [max(1-prob, prob) for prob in probabilities]  # Confidence = max(prob, 1-prob)\n",
    "})\n",
    "\n",
    "print(f\"Test samples: {len(results)}\")\n",
    "print(f\"Ham: {sum(results['prediction'] == 0)}, Spam: {sum(results['prediction'] == 1)}\")\n",
    "\n",
    "# Test set visualizations and comparisons\n",
    "print(\"\\nTest summary\")\n",
    "print(\"---\")\n",
    "test_counts = results['prediction_label'].value_counts().reindex(['ham', 'spam'], fill_value=0)\n",
    "print(test_counts.rename(index={'ham': 'Ham', 'spam': 'Spam'}).to_dict())\n",
    "\n",
    "plt.figure(figsize=(6, 4))\n",
    "sns.barplot(x=test_counts.index.map({'ham': 'Ham', 'spam': 'Spam'}), y=test_counts.values, palette=['#4c72b0', '#dd8452'])\n",
    "plt.title('Test Predictions')\n",
    "plt.ylabel('Count')\n",
    "plt.tight_layout()\n",
    "plt.savefig('test_prediction_counts.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "# Compare validation vs test distributions\n",
    "val_counts_named = pd.Series(labels).value_counts().rename({0: 'Ham', 1: 'Spam'})\n",
    "test_counts_named = test_counts.rename(index={'ham': 'Ham', 'spam': 'Spam'})\n",
    "compare_df = pd.DataFrame({'Validation': val_counts_named, 'Test': test_counts_named}).fillna(0)\n",
    "compare_df.plot(kind='bar', figsize=(6, 4), color=['#55a868', '#c44e52'])\n",
    "plt.title('Validation vs Test Label Counts')\n",
    "plt.ylabel('Count')\n",
    "plt.tight_layout()\n",
    "plt.savefig('validation_vs_test_counts.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "# Probability distribution for test predictions\n",
    "test_prob_df = pd.DataFrame({'prob': results['spam_probability'], 'label': results['prediction_label'].map({'ham': 0, 'spam': 1})})\n",
    "test_prob_df['label_name'] = test_prob_df['label'].map({0: 'Ham', 1: 'Spam'})\n",
    "plt.figure(figsize=(8, 5))\n",
    "sns.kdeplot(data=test_prob_df, x='prob', hue='label_name', fill=True, common_norm=False, palette=['#4c72b0', '#dd8452'], alpha=0.4)\n",
    "plt.title('Test Spam Probability Distribution by Predicted Label')\n",
    "plt.xlabel('Spam probability')\n",
    "plt.ylabel('Density')\n",
    "plt.savefig('test_probability_density.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "# Display a few sample predictions\n",
    "pd.options.display.max_colwidth = 120\n",
    "display(results[['text', 'prediction_label', 'spam_probability', 'confidence']].head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e08b1eb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_file = 'spam_predictions.csv'\n",
    "results.to_csv(output_file, index=False)\n",
    "\n",
    "# Summary with essential columns only\n",
    "summary_results = results[['prediction_label', 'spam_probability', 'confidence']].copy()\n",
    "summary_results.to_csv('spam_predictions_summary.csv', index=False)\n",
    "\n",
    "training_df = pd.DataFrame(stats)\n",
    "training_df.to_csv('training_statistics.csv', index=False)\n",
    "\n",
    "val_summary = pd.DataFrame({\n",
    "    'metric': ['accuracy', 'precision', 'recall', 'f1', 'roc_auc'],\n",
    "    'value': [acc, prec, rec, f1, auc]\n",
    "})\n",
    "val_summary.to_csv('validation_metrics.csv', index=False)\n",
    "\n",
    "# Generate model report with configuration and metrics\n",
    "report = f\"\"\"BERT Spam Email Detector - Model Report\\n\n",
    "\n",
    "Model Configuration:\n",
    "- Base Model: bert-base-uncased\n",
    "- Number of Classes: 2 (Ham=0, Spam=1)\n",
    "- Max Sequence Length: 128\n",
    "- Training Parameters: lr={lr}, epochs={epochs}, batch_size={batch_size}\n",
    "\n",
    "Dataset Information:\n",
    "- Total Training Samples: {len(X_train)}\n",
    "- Total Validation Samples: {len(X_val)}\n",
    "- Training Ham/Spam Ratio: {np.bincount(y_train)[0]}/{np.bincount(y_train)[1]}\n",
    "- Validation Ham/Spam Ratio: {np.bincount(y_val)[0]}/{np.bincount(y_val)[1]}\n",
    "\n",
    "Final Performance Metrics:\n",
    "- Accuracy: {acc:.4f}\n",
    "- Precision: {prec:.4f}\n",
    "- Recall: {rec:.4f}\n",
    "- F1 Score: {f1:.4f}\n",
    "- ROC-AUC: {auc:.4f}\n",
    "\n",
    "Test Set Predictions:\n",
    "- Total Test Emails: {len(results)}\n",
    "- Predicted Ham: {sum(results['prediction'] == 0)}\n",
    "- Predicted Spam: {sum(results['prediction'] == 1)}\n",
    "- Average Confidence: {results['confidence'].mean():.4f}\n",
    "\n",
    "Files Generated:\n",
    "- Predictions: {output_file}\n",
    "- Summary: spam_predictions_summary.csv\n",
    "- Training Stats: training_statistics.csv\n",
    "- Validation Metrics: validation_metrics.csv\n",
    "- Confusion Matrix: confusion_matrix.png\n",
    "- Training History: training_history.png\n",
    "- Validation Curves: validation_curves.png\n",
    "- Validation Probability Density: validation_probability_density.png\n",
    "- Test Prediction Counts: test_prediction_counts.png\n",
    "- Validation vs Test Counts: validation_vs_test_counts.png\n",
    "- Test Probability Density: test_probability_density.png\n",
    "\"\"\"\n",
    "\n",
    "with open('model_report.txt', 'w') as f:\n",
    "    f.write(report)\n",
    "\n",
    "print(\"Saved: spam_predictions.csv, spam_predictions_summary.csv, training_statistics.csv, model_report.txt\")\n",
    "\n",
    "if IN_COLAB:\n",
    "    print(\"\\\\nDownloading results\")\n",
    "    print(\"---\")\n",
    "    import zipfile\n",
    "    from google.colab import files\n",
    "    \n",
    "    # Package all results into zip file\n",
    "    with zipfile.ZipFile('results.zip', 'w') as zipf:\n",
    "        for f in ['spam_predictions.csv', 'spam_predictions_summary.csv', 'training_statistics.csv', \n",
    "                  'validation_metrics.csv', 'model_report.txt', 'confusion_matrix.png', 'training_history.png',\n",
    "                  'validation_curves.png', 'validation_probability_density.png',\n",
    "                  'test_prediction_counts.png', 'validation_vs_test_counts.png',\n",
    "                  'test_probability_density.png']:\n",
    "            if os.path.exists(f):\n",
    "                zipf.write(f)\n",
    "    \n",
    "    files.download('results.zip')\n",
    "    print(\"Downloaded: results.zip\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
