import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split, GridSearchCV
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.linear_model import LogisticRegression
from sklearn.pipeline import Pipeline
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, confusion_matrix, roc_curve
import matplotlib.pyplot as plt
import seaborn as sns
#Load Data 
try:
    df_train1 = pd.read_csv('project/spam_email/dataset/spam_train1.csv', encoding='latin-1')
    df_train2 = pd.read_csv('project/spam_email/dataset/spam_train2.csv')
    df_test = pd.read_csv('project/spam_email/dataset/spam_test.csv')
    print("Data files loaded successfully from the 'dataset' folder.")
except FileNotFoundError:
    print("Error: Ensure the 'dataset' folder exists and contains all CSV files.")
    exit()


# Standardize column names for training set 1 (v1 -> label, v2 -> text)
df_train1_clean = df_train1[['v1', 'v2']].rename(columns={'v1': 'label', 'v2': 'text'})

# Select relevant columns for training set 2 (label, text)
df_train2_clean = df_train2[['label', 'text']]

# Combine training data
df_train_combined = pd.concat([df_train1_clean, df_train2_clean], ignore_index=True)

# Create numerical target variable (0 for 'ham', 1 for 'spam')
df_train_combined['label_num'] = df_train_combined['label'].map({'ham': 0, 'spam': 1})

# Clean test data column name (message -> text)
df_test_clean = df_test.rename(columns={'message': 'text'})

# Define feature and target variables
X = df_train_combined['text']
y = df_train_combined['label_num']
X_test_final = df_test_clean['text']

# Split combined training data for training (used for CV) and validation (used for final metrics)
X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)
print(f"Combined Training set size: {len(X)}")
print(f"Validation set size: {len(X_val)}")

logreg_pipeline = Pipeline([
    ('tfidf', TfidfVectorizer(stop_words='english', lowercase=True)),
    ('clf', LogisticRegression(solver='liblinear', random_state=42))
])

print("ML pipeline (TF-IDF -> Logistic Regression) defined")

# Define parameter grid for tuning
param_grid = {
    # TF-IDF parameters
    'tfidf__max_df': [0.9, 0.95], # Ignore terms that appear in more than this fraction
    'tfidf__min_df': [2, 5],     # Ignore terms that appear in fewer than this number of documents
    # Logistic Regression parameter
    'clf__C': [1, 10, 100]       # C is the inverse of regularization strength
}

# Use GridSearchCV with 5-fold cross-validation, optimizing for F1-score
grid_search = GridSearchCV(logreg_pipeline, param_grid, cv=5, scoring='f1', n_jobs=1, verbose=0)

# Fit GridSearchCV on the full training data (X, y)
print("\nStarting Hyperparameter Tuning...")
grid_search.fit(X, y)

# Select the best model
best_logreg_model = grid_search.best_estimator_

print("\n--- Hyperparameter Tuning Results ---")
print(f"Best parameters found: {grid_search.best_params_}")
print(f"Best F1-Score from cross-validation: {grid_search.best_score_:.4f}")


y_val_pred = best_logreg_model.predict(X_val)
y_val_proba = best_logreg_model.predict_proba(X_val)[:, 1]

# Calculate metrics
accuracy = accuracy_score(y_val, y_val_pred)
precision = precision_score(y_val, y_val_pred)
recall = recall_score(y_val, y_val_pred)
f1 = f1_score(y_val, y_val_pred)
roc_auc = roc_auc_score(y_val, y_val_proba)

print("\n--- Final Model Evaluation (Tuned Logistic Regression) ---")
print(f"Accuracy: {accuracy:.4f}")
print(f"Precision: {precision:.4f}")
print(f"Recall: {recall:.4f}")
print(f"F1-Score: {f1:.4f}")
print(f"ROC-AUC: {roc_auc:.4f}")

# Plot ROC-AUC Curve
fpr, tpr, _ = roc_curve(y_val, y_val_proba)
plt.figure(figsize=(8, 6))
plt.plot(fpr, tpr, color='red', lw=2, label=f'ROC curve (area = {roc_auc:.4f})')
plt.plot([0, 1], [0, 1], color='gray', lw=2, linestyle='--')
plt.title('Receiver Operating Characteristic (ROC) Curve')
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.legend(loc="lower right")
plt.savefig("roc_curve_logreg.png")
plt.close()
print("Saved ROC Curve: roc_curve_logreg.png")

# Plot Confusion Matrix
cm = confusion_matrix(y_val, y_val_pred)
plt.figure(figsize=(7, 7))
sns.heatmap(cm, annot=True, fmt='d', cmap='Reds',
            xticklabels=['Ham (0)', 'Spam (1)'],
            yticklabels=['Ham (0)', 'Spam (1)'])
plt.ylabel('True Label')
plt.xlabel('Predicted Label')
plt.title('Confusion Matrix (Logistic Regression)')
plt.savefig("confusion_matrix_logreg.png")
plt.close()
print("Saved Confusion Matrix: confusion_matrix_logreg.png")print("Saved Confusion Matrix: confusion_matrix_logreg.png")

test_predictions_num = best_logreg_model.predict(X_test_final)
test_predictions_label = ['spam' if p == 1 else 'ham' for p in test_predictions_num]

# Create the output DataFrame
df_predictions = pd.DataFrame({
    'text': X_test_final,
    'predicted_label_num': test_predictions_num,
    'predicted_label': test_predictions_label
})

df_predictions.to_csv('spam_test_predictions.csv', index=False)
print("\nFinal test predictions saved to spam_test_predictions.csv")